# Amino Acid Classifier Neural Network
## The Neural Network
This network predicts the amino acid sequence based on an initial codon list (essentially automating the transcription and translation process) using  a built-in dataset. The network is fully densely connected and uses a sparse categorical crossentropy loss function since the dataset is categorical (the model predicts one of the possible protein classes). The neural network uses a standard Adam optimizer with a learning rate of 0.001. The model's architecture contains:
- 1 Input Layer (with 3 input neurons)
- 12 Hidden Layers (with either 128, 256, or 1024 neurons and a standard ReLU activation function)
- 1 Output Layer (with 22 output neurons)

Feel free to hyperparameter tune the model or experiment with the dataset!

## The Dataset
The data is generated within the python file (rather than being downloaded from an external source) and consists of sets of 64 codons (there are 64 to represent all codons equally since there are 64 unique codons). Each codon is represented as a list of [*base*, *base*, *base*] and is an input (x value) that is fed into the neural network; hence the input shape of the network is 3. Labels (y values) are generated by transcribing and translating those codons into their correct classes (each class is a protein). Each class is represented as [*number*] where *number* is some value between 0 and 21 (inclusive) since there are 22 amino acids in the dataset. This is why the output layer of the network has 22 neurons.

#### Dataset Generation Process Example 
1. Get untranscripted and untranslated codon
      - TAC
2. Encode the codon (this is the input fed into the neural network)
      -  [2, 1, 4] (T : 2, A : 1, C : 4)
3. Transcribe the codon (this is a hidden step to achieve a more accurate label; the neural network does not "see" this step)
      - TAC ==> AUG
      - [2, 1, 4] ==> [1, 5, 3]
4. Translate the codon into a class (this is the output the neural network trains on)
      - AUG ==> Methionine
      - [1, 5, 3] ==> [0] 
####
Since the dataset is generated within the file rather than being downloaded from a source, no data splitting is needed since the user can generate as many sets of codons and corresponding outputs as they please for training and validation sets with the **generate_codon_inputs()** function (which generates inputs) and the **transcription_and_translation_labels()** function (which generates corresponding labels for the inputs of the **generate_codon_inputs()** function. Note that the **generate_codon_inputs()** function takes a parameter **num_sets**, which determines how many sets of 64 codons the neural network views if (for whatever reason) one wanted to generate more than on set. For example, the code **generate_codon_inputs(10)** will result in 10 sets of 64 codons and therefore 640 individual (though not unique since there are 10 sets) data points. The result of **generate_codon_inputs()** can be passed as a parameter into **transcription_and_translation_labels()** to generate labels for the sets of 64 codons.

More information about how transcription and translation work can be found here: https://learn.genetics.utah.edu/content/basics/transcribe/.

## Libraries
This neural network was created with the assistance of the Tensorflow library.
- Tensorflow's Website: https://www.tensorflow.org/
- Tensorflow Installation Instructions: https://www.tensorflow.org/install
